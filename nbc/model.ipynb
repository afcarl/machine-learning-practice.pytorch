{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "In this exercise, I will implement binary NBCs for solving the Kaggle Cats vs. Dogs dataset.\n",
    "\n",
    "### Data \n",
    "\n",
    "Download the dataset from [here](https://www.kaggle.com/c/dogs-vs-cats/data) and unzip it in a subdirectory called `./data/`.\n",
    "\n",
    "### Model\n",
    "\n",
    "We will use [torchvision](https://pytorch.org/docs/master/torchvision/) pre-trained ResNet to extract features and train an NBC. Since we have real-valued features, we will use a simple Gaussian distribution to represent the class-conditional density $p(x | y = \\text{\\{cat,dog\\}}, \\theta) = \\Pi_{j=1}^D \\mathcal{N}(x_j | \\mu_{jc} , \\sigma_{jc}^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 1,\n",
    "    'num_workers': 0,\n",
    "    'feature_dim': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogsCatsDataset(Dataset):\n",
    "    def __init__(self, is_train, transform):\n",
    "        self.is_train = is_train\n",
    "        self.transform = transform\n",
    "        if self.is_train:\n",
    "            self.data_dir = Path(\"data/catdog/train/\")\n",
    "        else:\n",
    "            self.data_dir = Path(\"data/catdog/test1/\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        if self.is_train:\n",
    "            return 25000\n",
    "        else:\n",
    "            return 12500\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            if idx > 12499:\n",
    "                animal = \"dog\"\n",
    "                label = 1\n",
    "            else:\n",
    "                animal = \"cat\"\n",
    "                label = 0\n",
    "            file_name = animal + \".{}.jpg\".format(int(idx % 12500))\n",
    "            img_name = self.data_dir / file_name\n",
    "            img = io.imread(img_name)\n",
    "            img = self.transform(img)\n",
    "            sample = {'image': img, 'label': label}\n",
    "            return sample\n",
    "        else:\n",
    "            img_name = self.data_dir / \"{}.jpg\".format(idx+1)\n",
    "            img = io.imread(img_name)\n",
    "            img = self.transform(img)\n",
    "            sample = {'image': img}\n",
    "            return sample\n",
    "\n",
    "# Resize image to 256x256, then randomly crop to 224x224,\n",
    "# then normalize pixel values\n",
    "dataset = DogsCatsDataset(is_train=True, transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.RandomCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "]))\n",
    "\n",
    "N = 25000\n",
    "indices = np.arange(N)\n",
    "np.random.shuffle(indices)\n",
    "splitpoint = int(0.9 * N)\n",
    "train_idx = indices[:splitpoint]\n",
    "val_idx = indices[splitpoint:]\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "val_sampler = torch.utils.data.sampler.SubsetRandomSampler(val_idx)\n",
    "train_loader = DataLoader(dataset, batch_size=global_params['batch_size'],\n",
    "                         sampler=train_sampler, num_workers=global_params['num_workers'])\n",
    "val_loader = DataLoader(dataset, batch_size=global_params['batch_size'],\n",
    "                       sampler=val_sampler, num_workers=global_params['num_workers'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run our NBC, lets take a look at applying PCA to the features extracted by the pre-trained resnet18 model, since binary NBC has O(2D) parameters where D is the feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "gpu = torch.device(\"cuda:0\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        resnet = torchvision.models.resnet18(pretrained=True)\n",
    "        self.model = nn.Sequential(\n",
    "            resnet.conv1,\n",
    "            resnet.bn1,\n",
    "            resnet.relu,\n",
    "            resnet.maxpool,\n",
    "            resnet.layer1,\n",
    "            resnet.layer2,\n",
    "            resnet.layer3,\n",
    "            resnet.layer4,\n",
    "            resnet.avgpool\n",
    "        ) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out.view(out.size(0), -1)\n",
    "    \n",
    "resnet = ResNet()\n",
    "resnet = resnet.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693d7d12a2504c408719861e7599cee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# We need to create a 250x512 ndarray,\n",
    "# where dim1 is samples and sim2 is feature dim\n",
    "featurized = []\n",
    "val_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        x = batch['image']\n",
    "        val_labels.append(batch['label'])\n",
    "        x = x.to(gpu)\n",
    "        features = resnet(x)\n",
    "        featurized.append(features.to(cpu))\n",
    "X = torch.stack(featurized).squeeze()\n",
    "val_labels = torch.stack(val_labels).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` is a 2500 x 512 torch Tensor that we will now apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2500, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components='mle', random_state=None,\n",
       "  svd_solver='full', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "\n",
    "pca = PCA(n_components='mle', svd_solver='full')\n",
    "pca.fit(X.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02547279 0.01203371 0.00986715 0.00862709 0.00732145 0.00657297\n",
      " 0.0058737  0.00545661 0.00497204 0.00471083 0.00454205 0.00441003\n",
      " 0.0043314  0.00423256 0.00419967 0.00418377 0.00411503 0.00407292\n",
      " 0.00398244 0.003938   0.00391782 0.00388331 0.00386301 0.00380951]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x187075d0c88>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x187075d0400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "plt.plot(pca.singular_values_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try using the top 10 singular values. Now, we can define our NBC and train it with MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNBC(nn.Module):\n",
    "    \"\"\"\n",
    "    An NBC has O(DC) parameters, where \n",
    "    D is the feature dimension and C \n",
    "    is the number of classes.  \n",
    "    \n",
    "    Here, D = feature_dim and C = 2\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim):\n",
    "        super(GaussianNBC, self).__init__()\n",
    "        # mean and variance of the Gaussian for each feature \n",
    "        # dimension of each class\n",
    "        self.feature_dim = feature_dim\n",
    "        # [class, mean, var]\n",
    "        self.params = torch.zeros(2, 2, feature_dim)\n",
    "        # uninformative prior\n",
    "        self.log_priors = torch.log(torch.FloatTensor([0.5])) * torch.ones(2)\n",
    "        self.eps = torch.FloatTensor([1e-6])\n",
    "        self.c = torch.log(torch.FloatTensor([2 * np.pi]))\n",
    "        \n",
    "    def log_normal(self, x, c):\n",
    "        \"\"\"\n",
    "        compute log-likelihood of x, a [feature_dim] vector\n",
    "        under N(x | \\mu_c, \\sigma^2_c)\n",
    "        \n",
    "        returns a feature_dim vector of log-likelihoods\n",
    "        \"\"\"\n",
    "        return (-0.5 * (self.c + torch.log(self.params[c, 1, :]))) + \\\n",
    "                       (-0.5 * (1./(self.params[c, 1, :] + self.eps)) * \\\n",
    "                        ((x - self.params[c, 0, :]) ** 2).float())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: featurized image to classify after training\n",
    "        \"\"\"\n",
    "        preds = self.log_priors\n",
    "        preds[0] = preds[0] + torch.sum(self.log_normal(x, 0))\n",
    "        preds[1] = preds[1] + torch.sum(self.log_normal(x, 1))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c362f8ed5994a8c94584f65e3aa3759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pca_train = PCA(n_components=global_params['feature_dim'], svd_solver='randomized')\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        x = batch['image']\n",
    "        x = x.to(gpu)\n",
    "        features = resnet(x)\n",
    "        data.append(features.to(cpu))\n",
    "        labels.append(batch['label'])\n",
    "# 22500 x 512\n",
    "data = torch.stack(data).squeeze()\n",
    "# 22500\n",
    "labels = torch.stack(labels).squeeze()\n",
    "# 22500 x feature_dim\n",
    "data_reduced = pca_train.fit_transform(data)\n",
    "data_reduced = torch.from_numpy(data_reduced).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       [[-0.0034, -6.9734,  0.8322,  1.9554,  0.2579, -0.1051, -0.2677,\n",
      "         -0.3304, -0.0927,  0.0387],\n",
      "        [ 2.1051,  0.3843,  0.7289,  0.7536,  0.4674,  0.4535,  0.4121,\n",
      "          0.3521,  0.3493,  0.3245]])\n",
      "tensor(1.00000e-02 *\n",
      "       [[ 0.0034,  6.9796, -0.8329, -1.9571, -0.2581,  0.1052,  0.2679,\n",
      "          0.3307,  0.0928, -0.0387],\n",
      "        [ 2.0516,  0.5936,  0.9105,  0.5983,  0.7168,  0.5998,  0.5271,\n",
      "          0.4892,  0.3847,  0.3363]])\n"
     ]
    }
   ],
   "source": [
    "# Fit the GaussianNBC with MLE\n",
    "gauss_nbc = GaussianNBC(global_params['feature_dim'])\n",
    "\n",
    "# the MLE estimate for the parameters given the dataset\n",
    "# is just the MLE estimate of a Gaussian in each feature dimension, \n",
    "# i.e., x_hat_j = 1/N sum_i=1 to N (x_j) and \n",
    "# \\sigma^2_hat_j = 1/(N-1) sum_i=1 to N (x_j - x_hat_j) ** 2\n",
    "\n",
    "# select each class using `labels`\n",
    "#data_reduced = torch.from_numpy(data_reduced).float()\n",
    "labels = labels.byte()  \n",
    "flipped_labels = torch.abs(1 - labels)\n",
    "for j in range(global_params['feature_dim']):\n",
    "    gauss_nbc.params[1, 0, j] = torch.mean(data_reduced[labels, j])\n",
    "    gauss_nbc.params[1, 1, j] = torch.var(data_reduced[labels, j])\n",
    "    gauss_nbc.params[0, 0, j] = torch.mean(data_reduced[flipped_labels, j])\n",
    "    gauss_nbc.params[0, 1, j] = torch.var(data_reduced[flipped_labels, j])   \n",
    "    \n",
    "print(gauss_nbc.params[0,:,:10])\n",
    "print(gauss_nbc.params[1,:,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1777f98dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_data(model, label1='cat', label2='dog', d=None, y=None):\n",
    "    plt.figure()\n",
    "    plt.scatter(np.arange(global_params['feature_dim']), model.params[0,0].numpy(), c='r', label=label1)\n",
    "    plt.scatter(np.arange(global_params['feature_dim']), model.params[1,0].numpy(), c='b', label=label2)\n",
    "    if d is not None and y is not None:\n",
    "        plt.scatter(np.arange(global_params['feature_dim']), d, c='g', label=y)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040424b5953745e3a6870f5a390f9a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=22500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gaussian NBC training accuracy: 0.5004 %\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "N = 22500\n",
    "labels = labels.long()\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(N)):\n",
    "        x = data_reduced[idx]\n",
    "        y = labels[idx]\n",
    "        _, pred = torch.max(gauss_nbc(x), 0)\n",
    "        if pred == y:\n",
    "            num_correct += 1\n",
    "print(\"Gaussian NBC training accuracy: {} %\".format(num_correct / N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Guassian NBC is not able to do much with this dataset! :( Not too surprising.\n",
    "Just to see what happens, lets train LogReg on the full featurized dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = logreg.score(data, labels)\n",
    "print('logreg training accuracy: {} %'.format(training_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('logreg validation accuracy: {} %'.format(logreg.score(X, val_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titanic dataset\n",
    "\n",
    "Let's try a simpler dataset so we can get actual interesting results out of our Gaussian NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "df = pandas.read_csv(Path('data/titanic/train.csv'))\n",
    "# Throw out the name\n",
    "df = df.drop(['Name', 'Cabin', 'PassengerId', 'Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "y = df['Survived'].copy()\n",
    "df = df.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(x, upper, lower):\n",
    "    y = (x - x.min()) / (x.max() - x.min())\n",
    "    return y * (upper - lower) + lower\n",
    "\n",
    "# normalize entries\n",
    "df['Pclass'] = df['Pclass'].astype('category').cat.codes\n",
    "df['Pclass'] = min_max_scaling(df['Pclass'],-1,1)\n",
    "df['Sex'] = df['Sex'].astype('category').cat.codes\n",
    "df['Sex'] = min_max_scaling(df['Sex'],-1,1)\n",
    "df['Age'] = df['Age'].astype('category').cat.codes\n",
    "df['Age'] = min_max_scaling(df['Age'],-1,1)\n",
    "df['SibSp'] = df['SibSp'].astype('category').cat.codes\n",
    "df['SibSp'] = min_max_scaling(df['SibSp'],-1,1)\n",
    "df['Parch'] = df['Parch'].astype('category').cat.codes\n",
    "df['Parch'] = min_max_scaling(df['Parch'],-1,1)\n",
    "df['Fare'] = df['Fare'].astype('category').cat.codes\n",
    "df['Fare'] = min_max_scaling(df['Fare'],-1,1)\n",
    "df['Embarked'] = df['Embarked'].astype('category').cat.codes\n",
    "df['Embarked'] = min_max_scaling(df['Embarked'],-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.356322</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.853211</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.172414</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.651376</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218391</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.706422</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.080460</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.495413</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.080460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.688073</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.586207</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.467890</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.018349</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.440367</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.201835</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.701149</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.100917</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.425287</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.688073</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.195402</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.238532</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.752294</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.609195</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.155963</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.174312</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110092</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.080460</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055046</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.034483</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339450</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540230</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.697248</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.321101</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.018349</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.172414</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.247706</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.990826</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.218391</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119266</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.885057</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.761468</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.287356</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.477064</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.688073</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.471264</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.110092</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.356322</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.935780</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.517241</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.100917</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.287356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.082569</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.264368</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.284404</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.422018</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.448276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.045872</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.287356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339450</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.287356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339450</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.302752</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.449541</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.816092</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.440367</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.218391</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.715596</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.425287</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.486239</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.011494</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.981651</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.425287</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.587156</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.018349</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540230</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871560</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.425287</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.486239</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.715596</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.655172</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.770642</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.055046</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.011494</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.715596</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.356322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.449541</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.908257</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.195402</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>-0.174312</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.339450</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.192661</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.218391</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.192661</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.788991</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  Sex       Age  SibSp     Parch      Fare  Embarked\n",
       "0      -1.0 -1.0  0.356322    0.6  1.000000  0.853211      -1.0\n",
       "1       1.0  1.0 -0.172414    0.6  1.000000 -0.651376       1.0\n",
       "2      -1.0  1.0  0.218391    1.0  1.000000  0.706422      -1.0\n",
       "3       1.0  1.0 -0.080460    0.6  1.000000 -0.495413      -1.0\n",
       "4      -1.0 -1.0 -0.080460    1.0  1.000000  0.688073      -1.0\n",
       "6       1.0 -1.0 -0.586207    1.0  1.000000 -0.467890      -1.0\n",
       "7      -1.0 -1.0  0.862069   -0.2  0.666667  0.018349      -1.0\n",
       "8      -1.0  1.0  0.195402    1.0  0.333333  0.440367      -1.0\n",
       "9       0.0  1.0  0.586207    0.6  1.000000 -0.201835       1.0\n",
       "10     -1.0  1.0  0.816092    0.6  0.666667  0.137615      -1.0\n",
       "11      1.0  1.0 -0.701149    1.0  1.000000 -0.100917      -1.0\n",
       "12     -1.0 -1.0  0.425287    1.0  1.000000  0.688073      -1.0\n",
       "13     -1.0 -1.0 -0.195402    0.6 -0.666667 -0.238532      -1.0\n",
       "14     -1.0  1.0  0.586207    1.0  1.000000  0.752294      -1.0\n",
       "15      0.0  1.0 -0.609195    1.0  1.000000  0.155963      -1.0\n",
       "16     -1.0 -1.0  0.862069   -0.6  0.666667 -0.174312       0.0\n",
       "18     -1.0  1.0  0.057471    0.6  1.000000  0.110092      -1.0\n",
       "20      0.0 -1.0 -0.080460    1.0  1.000000 -0.055046      -1.0\n",
       "21      0.0 -1.0 -0.034483    1.0  1.000000  0.339450      -1.0\n",
       "22     -1.0  1.0  0.540230    1.0  1.000000  0.697248       0.0\n",
       "23      1.0 -1.0  0.172414    1.0  1.000000 -0.321101      -1.0\n",
       "24     -1.0  1.0  0.724138   -0.2  0.666667  0.018349      -1.0\n",
       "25     -1.0  1.0 -0.172414    0.6 -0.666667 -0.247706      -1.0\n",
       "27      1.0 -1.0  0.448276   -0.2  0.333333 -0.990826      -1.0\n",
       "30      1.0 -1.0 -0.218391    1.0  1.000000 -0.119266       1.0\n",
       "33      0.0 -1.0 -0.885057    1.0  1.000000  0.458716      -1.0\n",
       "34      1.0 -1.0  0.172414    0.6  1.000000 -0.761468       1.0\n",
       "35      1.0 -1.0 -0.287356    0.6  1.000000 -0.477064      -1.0\n",
       "37     -1.0 -1.0  0.379310    1.0  1.000000  0.688073      -1.0\n",
       "38     -1.0  1.0  0.471264    0.2  1.000000  0.110092      -1.0\n",
       "..      ...  ...       ...    ...       ...       ...       ...\n",
       "856     1.0  1.0 -0.356322    0.6  0.666667 -0.935780      -1.0\n",
       "857     1.0 -1.0 -0.517241    1.0  1.000000 -0.100917      -1.0\n",
       "858    -1.0  1.0  0.287356    1.0  0.000000  0.082569       1.0\n",
       "860    -1.0 -1.0 -0.264368    0.2  1.000000  0.284404      -1.0\n",
       "861     0.0 -1.0  0.379310    0.6  1.000000  0.422018      -1.0\n",
       "862     1.0  1.0 -0.448276    1.0  1.000000 -0.045872      -1.0\n",
       "864     0.0 -1.0  0.287356    1.0  1.000000  0.339450      -1.0\n",
       "865     0.0  1.0 -0.287356    1.0  1.000000  0.339450      -1.0\n",
       "866     0.0  1.0  0.195402    0.6  1.000000  0.302752       1.0\n",
       "867     1.0 -1.0  0.057471    1.0  1.000000 -0.449541      -1.0\n",
       "869    -1.0 -1.0  0.816092    0.6  0.666667  0.440367      -1.0\n",
       "870    -1.0 -1.0  0.218391    1.0  1.000000  0.715596      -1.0\n",
       "871     1.0  1.0 -0.425287    0.6  0.666667 -0.486239      -1.0\n",
       "872     1.0 -1.0 -0.011494    1.0  1.000000  0.981651      -1.0\n",
       "873    -1.0 -1.0 -0.425287    1.0  1.000000  0.587156      -1.0\n",
       "874     0.0  1.0  0.172414    0.6  1.000000 -0.018349       1.0\n",
       "875    -1.0  1.0  0.540230    1.0  1.000000  0.871560       1.0\n",
       "876    -1.0 -1.0  0.425287    1.0  1.000000  0.486239      -1.0\n",
       "877    -1.0 -1.0  0.448276    1.0  1.000000  0.715596      -1.0\n",
       "879     1.0  1.0 -0.655172    1.0  0.666667 -0.770642       1.0\n",
       "880     0.0  1.0  0.241379    1.0  0.666667 -0.055046      -1.0\n",
       "881    -1.0 -1.0 -0.011494    1.0  1.000000  0.715596      -1.0\n",
       "882    -1.0  1.0  0.356322    1.0  1.000000  0.449541      -1.0\n",
       "883     0.0 -1.0  0.172414    1.0  1.000000  0.458716      -1.0\n",
       "884    -1.0 -1.0  0.241379    1.0  1.000000  0.908257      -1.0\n",
       "885    -1.0  1.0 -0.195402    1.0 -0.666667 -0.174312       0.0\n",
       "886     0.0 -1.0  0.195402    1.0  1.000000  0.339450      -1.0\n",
       "887     1.0  1.0  0.448276    1.0  1.000000 -0.192661      -1.0\n",
       "889     1.0 -1.0  0.218391    1.0  1.000000 -0.192661       1.0\n",
       "890    -1.0 -1.0  0.034483    1.0  1.000000  0.788991       0.0\n",
       "\n",
       "[712 rows x 7 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4858, -0.6981,  0.0822,  0.7896,  0.8781,  0.2929, -0.7123])\n",
      "tensor([ 0.1215,  0.3542,  0.1476,  0.8014,  0.8229, -0.1014, -0.4236])\n"
     ]
    }
   ],
   "source": [
    "feature_dim = df.shape[1]\n",
    "gauss_nbc_ = GaussianNBC(feature_dim=feature_dim)\n",
    "\n",
    "data = torch.from_numpy(df.values).float()\n",
    "y = torch.from_numpy(y.values).byte()\n",
    "flipped_y = torch.abs(1 - y)\n",
    "for j in range(feature_dim):\n",
    "    # survived = 1\n",
    "    gauss_nbc_.params[1, 0, j] = torch.mean(data[y, j])\n",
    "    gauss_nbc_.params[1, 1, j] = torch.var(data[y, j])\n",
    "    # survived = 0\n",
    "    gauss_nbc_.params[0, 0, j] = torch.mean(data[flipped_y, j])\n",
    "    gauss_nbc_.params[0, 1, j] = torch.var(data[flipped_y, j])\n",
    "    \n",
    "print(gauss_nbc_.params[0,0,:])\n",
    "print(gauss_nbc_.params[1,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2a8f395ea74f829345bad3bfa4e98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=712), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gaussian NBC training accuracy: 0.5702247191011236 %\n",
      "average log-likelihood: -2244.790283203125\n"
     ]
    }
   ],
   "source": [
    "# Training set accuracy\n",
    "num_correct = 0\n",
    "N = data.shape[0]\n",
    "y = y.long()\n",
    "logprobs = 0\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(N)):\n",
    "        x = data[idx]\n",
    "        y_ = y[idx]\n",
    "        p, pred = torch.max(gauss_nbc_(x), 0)\n",
    "        if pred == y_:\n",
    "            num_correct += 1\n",
    "        logprobs += p\n",
    "print(\"Gaussian NBC training accuracy: {} %\".format(num_correct / N))\n",
    "print(\"average log-likelihood: {}\".format(logprobs / N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better than random! For funsies let's also overfit the dataset with a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree training accuracy: 0.9859550561797753 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(data, y)\n",
    "\n",
    "print(\"Decision tree training accuracy: {} %\".format(clf.score(data, y)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
